{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jj/llmoptim/thesis_llm_collective_behavior_generation/scripts\n"
     ]
    }
   ],
   "source": [
    "%cd ../scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jj/miniforge3/envs/automode_llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from pipeline.pipeline import MLPipeline\n",
    "SCRIPT_PATH=\"./run_argos_with_vis.sh\"\n",
    "MODEL_PATH = \"../llm_training/demo_train_2024-12-23_12_automode_evaluated_concat_s14-s18_24-12-23_wtargetlights\"\n",
    "OUTPUT_PATH=\"dpo_rl_model\"\n",
    "NUM_SCORES_PER_RUN=3\n",
    "NUM_ROWS_PER_EPOCH=5\n",
    "NUM_EPOCHS=1\n",
    "SKELETON_TEMPLATE=\"../ressources/skeleton.argos\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "from swarm_descriptions.mission_elements import get_generators, MissionParams\n",
    "from swarm_descriptions.configfiles import config_to_string\n",
    "from swarm_descriptions.utils import truncate_floats\n",
    "from swarm_descriptions.configfiles import ET, Configurator\n",
    "\n",
    "def sample_dataset(n_rows = 10000, generators = get_generators()) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for _n in tqdm(range(n_rows)):\n",
    "        mission = MissionParams.sample(*generators)\n",
    "        conf = config_to_string(mission.configure())\n",
    "        conf = truncate_floats(conf)\n",
    "        desc = random.sample(mission.describe(),1)[0]  \n",
    "        desc = truncate_floats(desc)\n",
    "        rows.append({\"description\": desc, \"configuration\": conf, \"parameters\": mission})\n",
    "    dataset = pd.DataFrame(rows)\n",
    "    \n",
    "    return dataset\n",
    "# %%\n",
    "\n",
    "script_name = \"\"\n",
    "import tempfile\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def evaluate_configuration(argos,behavior_tree,script_path=\"./run_argos_with_vis.sh\",tmpfile=\"/tmp/vis.argos\"):\n",
    "    \"\"\"expects argos to include commented out visualization\"\"\"\n",
    "    \n",
    "    res = None\n",
    "        # Create a temporary file for the argos file\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.argos') as temp_file:\n",
    "        # Write the behavior_tree entry to the temporary file\n",
    "        temp_file.write(argos.encode('utf-8'))\n",
    "        temp_file_path = temp_file.name  # Get the path of the temporary file\n",
    "        \n",
    "    behavior_tree_args = behavior_tree.split()\n",
    "\n",
    "    # Prepare the command to run, including the temporary file and behavior_tree arguments\n",
    "    command = [script_path, \"--no-vis\", temp_file_path, tmpfile] + behavior_tree_args\n",
    "    try:\n",
    "        # Run the command and capture the output\n",
    "        result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "        \n",
    "        # Print the executed command\n",
    "        #print(f\"Executed: {' '.join(command)}\")\n",
    "        \n",
    "        # Get the output from the command\n",
    "        output = result.stdout\n",
    "        \n",
    "        # Print the output for debugging\n",
    "        #print(\"Command Output:\")\n",
    "        #print(output)\n",
    "        \n",
    "        # Extract the number from the line starting with \"Score\"\n",
    "        score_line = next((line for line in output.splitlines() if line.startswith(\"Score\")), None)\n",
    "        #print(f\"score line {score_line=}\")\n",
    "        if score_line:\n",
    "            # Use regex to extract the number from the score line\n",
    "            score = re.search(r'-?\\d+(\\.\\d+)?', score_line)\n",
    "            if score:\n",
    "                res = float(score.group())\n",
    "            else:\n",
    "                print(\"No score number found in the score line.\")\n",
    "        else:\n",
    "            print(\"No line starting with 'Score' found in the output.\")\n",
    "            \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred while executing the command: {e}\")\n",
    "    finally:\n",
    "        pass#os.remove(temp_file_path)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_prompt(llmin, llmout, tokenizer):\n",
    "        #f\"\\nNUMNODES={int(len(llmout.split(' '))/2.0)}\\n\"+\n",
    "        # f\"\\nsyntax example: {stx}\\n\"\n",
    "        # Specify the tree inside |BTSTART|<TREE>|BTEND| by starting the tree with --nroot.\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": llmin+\"\\nGenerate the behavior tree that achieves the objective of this mission.\"},\n",
    "            {\"role\": \"assistant\", \"content\": llmout},\n",
    "        ]\n",
    "\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, truncation=True, return_dict=False) # wraps text with special tokens depending on role (assitant or user)\n",
    "        return text\n",
    "    \n",
    "def perform_inference(txt):\n",
    "    txt = txt_prompt(txt, \"\", mlp.tokenizer)[:-5]\n",
    "    out = mlp.inference(txt, seq_len=1000, temperature=0.21)\n",
    "    res = None\n",
    "    try:\n",
    "        res = out.split(\"[/INST]\")[1]\n",
    "        res = res.split(\"</s>\")[0]\n",
    "    except:\n",
    "        res = None\n",
    "    #print(res)\n",
    "    return res\n",
    "\n",
    "def generate_prompt(sample, tokenizer):\n",
    "        return txt_prompt(sample[\"llm_input\"],sample[\"llm_output\"], tokenizer)\n",
    "\n",
    "def evaluate_tree(behavior_tree, mission):\n",
    "    # Check if behavior_tree is not None\n",
    "    if type(behavior_tree) != str or len(behavior_tree) == 0:\n",
    "        print(\"could not evaluate behavior_tree, it is empty\")\n",
    "        return None\n",
    "\n",
    "        # Execute the command\n",
    "    scores = []\n",
    "    for i in range(NUM_SCORES_PER_RUN):\n",
    "        score = evaluate_configuration(mission, behavior_tree, script_path=SCRIPT_PATH)\n",
    "        if score is not None:\n",
    "            scores.append(score)\n",
    "            \n",
    "    return np.mean(scores).item()  if len(scores) > 0 else None\n",
    "\n",
    "\n",
    "\n",
    "def add_config_to_dataset(df: pd.DataFrame, skeleton: ET.ElementTree):\n",
    "    result = []\n",
    "    for config_params in df[\"configuration\"]:\n",
    "        argos_config = config_params_to_argos_config(config_params, skeleton)        \n",
    "        result.append(argos_config)\n",
    "    df[\"argos\"] = result\n",
    "    return df  \n",
    "\n",
    "def config_params_to_argos_config(params: str, skeleton: ET.ElementTree):\n",
    "    xml = ET.fromstring(params)\n",
    "    config_tree = Configurator().convert_config_params(params=xml, skeleton_root=skeleton)\n",
    "    config = config_tree.getroot()\n",
    "    config = config_to_string(config)\n",
    "    xml = ET.fromstring(params)\n",
    "    config_tree = Configurator().convert_config_params(params=xml, skeleton_root=skeleton)\n",
    "    config = config_tree.getroot()\n",
    "    config = config_to_string(config)\n",
    "    return config\n",
    "\n",
    "def rescale_score(score, df, category):\n",
    "    df_cat = df[df.type == category]\n",
    "    min_score = min(df_cat.scores_bt1.min(), df_cat.scores_bt2.min())\n",
    "    max_score = max(df_cat.scores_bt1.max(), df_cat.scores_bt2.max())\n",
    "    #print(score,min_score, max_score,float(score-min_score),float(max_score - min_score))\n",
    "    if score is None or min_score is None or max_score is None or min_score == max_score:\n",
    "        return None\n",
    "    score_scaled = float(score-min_score)/float(max_score - min_score)\n",
    "    #print(score_scaled)\n",
    "    \n",
    "    return score_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:38<00:00,  6.34s/it]\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:13<00:00,  2.31s/it]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPipeline()\n",
    "mlp.prepare_model() # need both currently\n",
    "#mlp.prepare_model_from_path(path=MODEL_PATH)\n",
    "mlp.prepare_dpo_model(model_path=MODEL_PATH)\n",
    "\n",
    "sft_config_params = mlp.sft_default_config\n",
    "sft_config_params[\"num_train_epochs\"] = 12\n",
    "sft_config_params[\"learning_rate\"] = 2e-5\n",
    "sft_config_params[\"warmup_ratio\"] = 0.10\n",
    "sft_config_params[\"weight_decay\"] = 0.123\n",
    "sft_config_params[\"max_grad_norm\"] = 0.15\n",
    "sft_config_params[\"gradient_accumulation_steps\"] = 2\n",
    "sft_config_params[\"max_seq_length\"] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 161.17it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [00:24<01:37, 24.46s/it, current_item=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while executing the command: Command '['./run_argos_with_vis.sh', '--no-vis', '/tmp/tmp1r82c1md.argos', '/tmp/vis.argos', '--nroot', '3', '--nchildroot', '3', '--n0', '0', '--nchild0', '2', '--n00', '6', '--c00', '2', '--p00', '0.8013', '--n01', '5', '--a01', '2', '--p01', '0', '--n1', '0', '--nchild1', '2', '--n10', '6', '--c10', '1', '--p10', '0.9998', '--n11', '5', '--a11', '5', '--rep11', '3.5236', '--p11', '0', '--n2', '0', '--nchild2', '2', '--n20', '6', '--c20', '0', '--p20', '0.158', '--n21', '5', '--a21', '4', '--att21', '4.4574', '--p21', '0']' returned non-zero exit status 1.\n",
      "An error occurred while executing the command: Command '['./run_argos_with_vis.sh', '--no-vis', '/tmp/tmp2tcoy3pg.argos', '/tmp/vis.argos', '--nroot', '3', '--nchildroot', '3', '--n0', '0', '--nchild0', '2', '--n00', '6', '--c00', '2', '--p00', '0.8013', '--n01', '5', '--a01', '2', '--p01', '0', '--n1', '0', '--nchild1', '2', '--n10', '6', '--c10', '1', '--p10', '0.9998', '--n11', '5', '--a11', '5', '--rep11', '3.5236', '--p11', '0', '--n2', '0', '--nchild2', '2', '--n20', '6', '--c20', '0', '--p20', '0.158', '--n21', '5', '--a21', '4', '--att21', '4.4574', '--p21', '0']' returned non-zero exit status 1.\n",
      "An error occurred while executing the command: Command '['./run_argos_with_vis.sh', '--no-vis', '/tmp/tmpevj0tpg8.argos', '/tmp/vis.argos', '--nroot', '3', '--nchildroot', '3', '--n0', '0', '--nchild0', '2', '--n00', '6', '--c00', '2', '--p00', '0.8013', '--n01', '5', '--a01', '2', '--p01', '0', '--n1', '0', '--nchild1', '2', '--n10', '6', '--c10', '1', '--p10', '0.9998', '--n11', '5', '--a11', '5', '--rep11', '3.5236', '--p11', '0', '--n2', '0', '--nchild2', '2', '--n20', '6', '--c20', '0', '--p20', '0.158', '--n21', '5', '--a21', '4', '--att21', '4.4574', '--p21', '0']' returned non-zero exit status 1.\n",
      "An error occurred while executing the command: Command '['./run_argos_with_vis.sh', '--no-vis', '/tmp/tmp26kvznx3.argos', '/tmp/vis.argos', '--nroot', '3', '--nchildroot', '3', '--n0', '0', '--nchild0', '2', '--n00', '6', '--c00', '2', '--p00', '0.0976', '--n01', '5', '--a01', '2', '--p01', '0', '--n1', '0', '--nchild1', '2', '--n10', '6', '--c10', '1', '--p10', '0.997', '--n11', '5', '--a11', '5', '--rep11', '3.4087', '--p11', '0', '--n2', '0', '--nchild2', '2', '--n20', '6', '--c20', '0', '--p20', '0.9896', '--n21', '5', '--a21', '4', '--att21', '4.2025', '--p21', '0']' returned non-zero exit status 1.\n",
      "An error occurred while executing the command: Command '['./run_argos_with_vis.sh', '--no-vis', '/tmp/tmpbtnm0l7s.argos', '/tmp/vis.argos', '--nroot', '3', '--nchildroot', '3', '--n0', '0', '--nchild0', '2', '--n00', '6', '--c00', '2', '--p00', '0.0976', '--n01', '5', '--a01', '2', '--p01', '0', '--n1', '0', '--nchild1', '2', '--n10', '6', '--c10', '1', '--p10', '0.997', '--n11', '5', '--a11', '5', '--rep11', '3.4087', '--p11', '0', '--n2', '0', '--nchild2', '2', '--n20', '6', '--c20', '0', '--p20', '0.9896', '--n21', '5', '--a21', '4', '--att21', '4.2025', '--p21', '0']' returned non-zero exit status 1.\n",
      "An error occurred while executing the command: Command '['./run_argos_with_vis.sh', '--no-vis', '/tmp/tmp0tg7w58t.argos', '/tmp/vis.argos', '--nroot', '3', '--nchildroot', '3', '--n0', '0', '--nchild0', '2', '--n00', '6', '--c00', '2', '--p00', '0.0976', '--n01', '5', '--a01', '2', '--p01', '0', '--n1', '0', '--nchild1', '2', '--n10', '6', '--c10', '1', '--p10', '0.997', '--n11', '5', '--a11', '5', '--rep11', '3.4087', '--p11', '0', '--n2', '0', '--nchild2', '2', '--n20', '6', '--c20', '0', '--p20', '0.9896', '--n21', '5', '--a21', '4', '--att21', '4.2025', '--p21', '0']' returned non-zero exit status 1.\n",
      "generated 0\n",
      "scores computed and rescaled\n",
      "could not show scores\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "trl.trainer.sft_config.SFTConfig() got multiple values for keyword argument 'description'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 74\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not show scores\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#%%\u001b[39;00m\n\u001b[1;32m     73\u001b[0m  \u001b[38;5;66;03m# as this is done everytime the final version should be the one in the directory after exececution, I assume that training the same model twice works \u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msft_config_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#mlp.train_dpo(df, save_path=OUTPUT_PATH)\u001b[39;00m\n",
      "File \u001b[0;32m~/llmoptim/thesis_llm_collective_behavior_generation/pipeline/pipeline.py:316\u001b[0m, in \u001b[0;36mMLPipeline.train_model\u001b[0;34m(self, sft_config_params, save_path)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[38;5;66;03m# Set up the training arguments\u001b[39;00m\n\u001b[1;32m    305\u001b[0m         training_args_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: save_path,\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation_strategy\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    315\u001b[0m }\n\u001b[0;32m--> 316\u001b[0m         sft_arguments \u001b[38;5;241m=\u001b[39m SFTConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtraining_args_dict,\n\u001b[1;32m    317\u001b[0m                                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msft_config_params)\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# Initialize the SFTTrainer with the SFTConfig\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: trl.trainer.sft_config.SFTConfig() got multiple values for keyword argument 'description'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "df = sample_dataset(NUM_ROWS_PER_EPOCH)\n",
    "skeleton = ET.parse(SKELETON_TEMPLATE)\n",
    "df = add_config_to_dataset(df, skeleton)\n",
    "df[\"type\"] = df[\"parameters\"].map(lambda x: type(x.objective_params).__name__)\n",
    "df['scores_bt1'] = [None] * len(df)\n",
    "df['scores_bt2'] = [None] * len(df) \n",
    "df['bt1'] = [None] * len(df)\n",
    "df['bt2'] = [None] * len(df) \n",
    "df['scores_bt1_scaled'] = [None] * len(df)\n",
    "df['scores_bt2_scaled'] = [None] * len(df) \n",
    "\n",
    "progress_bar = tqdm(total=len(df))\n",
    "# Iterate through the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    progress_bar.set_postfix(current_item = index)\n",
    "    behavior_tree = perform_inference(row[\"description\"])\n",
    "    behavior_tree2 = perform_inference(row[\"description\"])\n",
    "    # print(behavior_tree)\n",
    "    # print(behavior_tree2)\n",
    "    mission = row[\"argos\"]\n",
    "    category = row[\"type\"]\n",
    "    \n",
    "    score1 = evaluate_tree(behavior_tree, mission)\n",
    "    score2 = evaluate_tree(behavior_tree2, mission)\n",
    "    df.at[index, \"scores_bt1\"] = score1\n",
    "    df.at[index, \"scores_bt2\"] = score2\n",
    "    \n",
    "    \n",
    "    #exit(1)\n",
    "    \n",
    "    df.at[index,\"bt1\"] = behavior_tree\n",
    "    df.at[index,\"bt2\"] = behavior_tree2\n",
    "\n",
    "    #print(scores,df.at[index,\"llm_scores\"])\n",
    "    # df.at[index,\"llm_scores\"] = scores\n",
    "    # df.at[index, \"llm_avg_score\"] = \n",
    "    progress_bar.write(f\"generated {index}\")\n",
    "    progress_bar.update(1)\n",
    "    # %%\n",
    "    #score1_scaled = rescale_score(score1,df,category)\n",
    "    #score2_scaled = rescale_score(score2,df,category)\n",
    "    #print(category, score1, score2, score1_scaled, score2_scaled)\n",
    "    #df.to_pickle(\"debug.pickle\")\n",
    "    df[\"scores_bt1_scaled\"] = df.apply(lambda row: rescale_score(row[\"scores_bt1\"], df, row[\"type\"]), axis=1)\n",
    "    df[\"scores_bt2_scaled\"] = df.apply(lambda row: rescale_score(row[\"scores_bt2\"], df, row[\"type\"]), axis=1)\n",
    "    df = df.dropna(subset=[\"scores_bt1_scaled\", \"scores_bt2_scaled\"])\n",
    "    print(f\"scores computed and rescaled\")\n",
    "    def choose_and_reject(row):\n",
    "        if row['scores_bt1_scaled'] > row['scores_bt2_scaled']:\n",
    "            return pd.Series({\n",
    "                'chosen': txt_prompt(row[\"description\"], row['bt1'], mlp.tokenizer),\n",
    "                'rejected': txt_prompt(row[\"description\"], row['bt2'], mlp.tokenizer),\n",
    "                'score_chosen': row['scores_bt1_scaled'],\n",
    "                'score_rejected': row['scores_bt2_scaled']\n",
    "            })\n",
    "        else:\n",
    "            return pd.Series({\n",
    "                'chosen':txt_prompt(row[\"description\"], row['bt2'], mlp.tokenizer),\n",
    "                'rejected': txt_prompt(row[\"description\"], row['bt1'], mlp.tokenizer),\n",
    "                'score_chosen': row['scores_bt2_scaled'],\n",
    "                'score_rejected': row['scores_bt1_scaled']\n",
    "            })\n",
    "\n",
    "    result = df.apply(choose_and_reject, axis=1)\n",
    "    df = pd.concat([df, result], axis=1)\n",
    "\n",
    "    try:\n",
    "        print(df[[\"chosen\", \"rejected\", \"score_chosen\",\"score_rejected\"]].head())\n",
    "    except:\n",
    "        print(\"could not show scores\")\n",
    "\n",
    "    #%%\n",
    "     # as this is done everytime the final version should be the one in the directory after exececution, I assume that training the same model twice works \n",
    "    mlp.train_model(df, sft_config_params)\n",
    "    #mlp.train_dpo(df, save_path=OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automode_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
