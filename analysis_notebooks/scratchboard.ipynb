{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../ressources/final_experiments/res_increasing_size/llm_evaluated_3000.pickle\", \"rb\") as file:\n",
    "    original_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-32584.43)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_df[original_df.type==\"Distribution\"].avg_score.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-32584.43)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../ressources/final_experiments/automode_datasets/df_increasing_size_validate.pickle\", \"rb\") as file:\n",
    "    original_df = pickle.load(file)\n",
    "    \n",
    "original_df[original_df.type==\"Distribution\"].avg_score.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../ressources/automode_evaluated_concat_s14s15s16_n300_24-12-18.pickle\", \"rb\") as file:\n",
    "    original_df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pipeline import CustomSFTTrainer\n",
    "\n",
    "# sft_trainer = CustomSFTTrainer()\n",
    "# trained_model, hf_sft_trainer, dataset_train = sft_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pipeline import CustomInference\n",
    "\n",
    "# generated_text = CustomInference(model=trained_model,tokenizer=sft_trainer.tokenizer).inference()\n",
    "# generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pipeline.utils import DEFAULT_GENERATE_PROMPT\n",
    "\n",
    "# DEFAULT_GENERATE_PROMPT(\"test\", \"\", sft_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import CustomDPOTrainer\n",
    "\n",
    "dpo_trainer = CustomDPOTrainer(learning_rate=1000.99, num_train_epochs=5)\n",
    "dpo_trained_model, hf_dpo_trainer, dataset_train = dpo_trainer.train()\n",
    "print(dpo_trained_model is None)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "from pipeline import CustomInference\n",
    "generated_text = CustomInference(model=dpo_trained_model,tokenizer=dpo_trainer.tokenizer).inference()\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model or tokenizer not set, loading default model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text not set, using dummy text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s>[INST] The environment features a rectangle with dimensions 5.57 x 2.19 x 1.75.There are the following lights in the arena: ((-0.25, 0.57)). 16 robots are evenly spaced around the central point, spanning a radius of 0.96 m. In the surroundings, there exists a circle at [-0.71, 0.76] with a radius of 0.25 meters, exhibiting a white color, and another at [0.78, -1.25] with a radius of 0.27 meters in black. The objective for the robots is to transfer items from the white initial location to the black circle. \\nGenerate the behavior tree that achieves the objective of this mission.[/INST] Sure, here's a basic behavior tree that could achieve the objective of transferring items from the white circle to the black circle:\\n\\n1. **Start**: This is the initial state where the robot is idle.\\n\\n2. **Detect White Circle**: The robot uses its sensors to detect the white circle. If the white circle is detected, it proceeds to the next state. If not, it continues to scan the environment.\\n\\n3. **Move to White Circle**: The robot moves towards the white circle. This could be a series of smaller movements or a direct path, depending on the robot's capabilities.\\n\\n4. **Pick Up Item**: The robot picks up the item from the white circle. This could involve a specific action or a series of actions, depending on the item's properties.\\n\\n5. **Detect Black Circle**: The robot uses its sensors to detect the black circle. If the black circle is detected, it proceeds to the next state. If not, it continues to scan the environment.\\n\\n6. **Move to Black Circle**: The robot moves towards the black circle. This could be a series of smaller movements or a direct path, depending on the robot's capabilities.\\n\\n7. **Drop Item**: The robot drops the item at the black circle. This could involve a specific action or a series of actions, depending on the item's properties.\\n\\n8. **End**: The robot has successfully transferred the item from the white circle to the black circle. It could then return to the start state to begin a new mission, or it could end the mission.</s>\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "generated_text = CustomInference().inference()\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model or tokenizer not set, loading default model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:22<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text not set, using dummy text\n"
     ]
    }
   ],
   "source": [
    "base_model = CustomInference().model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.models.mistral.modeling_mistral.MistralForCausalLM,\n",
       " peft.peft_model.PeftModelForCausalLM)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dpo_trained_model), type(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trained_model.save_pretrained(\"dpo_trained\")\n",
    "base_model.save_pretrained(\"base_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dpo_trainer.model.save_pretrained(\"hfmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dpo_trainer.save_model(\"sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.model.save_pretrained(\"unmerged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.utils import load_default_model\n",
    "\n",
    "model, tokenizer, lora,bnb = load_default_model()\n",
    "\n",
    "model.save_pretrained(\"base_model\")\n",
    "tokenizer.save_pretrained(\"base_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automode_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
